#saved some time - spatSample restrict to extent works but not to polygon
# check again at a later date if possible
# test.pts <- spatSample(bg.raster, nrow(loss.pts),
#                        method="random",
#                      #xy=T,
#                      as.points=T,
#                      ext = ext(roi),
#                      na.rm = T)
set.seed = 2022
bg.pts <- as.points(bg.raster) %>%
geom() %>%
as.data.frame() %>%
transmute(x,y) %>%
slice_sample(n = nrow(loss.pts))
## 3. Join loss and background points 1= loss and 0 = background
pts <- rbind(
loss.pts %>% mutate(loss = as.factor(1)),
bg.pts %>% mutate(loss = as.factor(0)))
##3.1 Convert to sf object
pts <- st_as_sf(pts, coords= c("x","y"))
model.name <- str_c("roi_",region,"_",
"agent_",agent,"_",
"yr_",yearmodelled,"_",
"samples_",nrow(loss.pts),"_",
"cv_",nfolds,"_",
"rep_",nreps,"_",
"mod_",nmod)
## 4.4 Covariates data
covariates <- rast(cov.path)
ifelse(
region == "state",
covariates,
covariates <- covariates %>% crop(roi) %>% mask(vect(roi))
)
print(str_c("No of covariates used for modelling: ", nlyr(covariates)))
#extract names from file_names
good.names <- str_extract(cov.path,pattern = "(?<=_)[^.]*(?=.)")
good.names
#Assign names now
names(covariates) <- good.names
#Join kd
## 5 Extract the covariate data into the training samples
df <- terra::extract(covariates,vect(pts), xy=T) %>%
data.frame() %>%
mutate(loss = as.factor(pts$loss)) %>%
dplyr::select(-ID)%>% #At this step check which points have NA -
drop_na() #mlr doesn't take na so remove if any column has NA
print(str_c("Data preparation complete for model:: ",model.name))
# #Plot the distribution of data
# df_long <- df %>% pivot_longer(
#   cols = - c(loss),
#   names_to = "variables") %>%
#     ggplot(aes(x = loss, y = value))+
#     geom_boxplot()+
#     facet_wrap(~variables, scales = "free_y")
#
# df_long
#
# boxplot_fun <- function(x,y){
#   ggplot(df, aes_string(x = x, y = y))+
#     geom_boxplot()
# }
#
# boxplot_fun (x = "elevation", y="loss")
###Get the coordinates for spatial partitioning
#coords <- df[ , c("lon","lat")]
################# STEP 2: MODELLING SECTION ###########################
print(str_c("Model training starting for ", agent, " in ", region, " bioregion"))
## Step 2.1 Create a classification task
task = mlr3spatiotempcv::TaskClassifST$new(
id = "xgboost_model",
backend = mlr3::as_data_backend(df),
target = "loss",
positive = "1",
extra_args = list(
coordinate_names = c("x", "y"), #Specify to use these columns as coordinates
coords_as_features = FALSE,
crs = "EPSG:3577") #Albers
)
levs = levels(task$truth())
## Step 2.2 Choose a learner and set predict output in probability
learner = mlr3::lrn("classif.xgboost",predict_type = "prob")
#booster = "gbtree",
#tree_method = "hist")
##Supposedly, specifying booster=gbtree and tree_method = "hist" in the learner was going to run xgboost faster
#tried on my laptop but it made the run slower, perhaps with GPU you can use "gpu_hist" sometime in the future
#To make sure the tuning doesn't stop if any model fails
learner$fallback = lrn("classif.xgboost", predict_type = "prob")
#set to use 4 CPUs
set_threads(learner, n = availableCores()-4)
#Check the parameters you can set
# learner$param_set$ids()
# learner$help()
##Step 2.2 Performance estimation level using resampling technique
#Nested Spatial CV
#Create a outer level resampling strategy
perf.level = mlr3::rsmp("repeated_spcv_coords", folds = nfolds, repeats = nreps)
#Create an inner level resampling strategy
tune.level = mlr3::rsmp("spcv_coords", folds = nfolds)
cvplot = autoplot(tune.level, task, c(1, 2, 3, 4, 5),
crs = 3577, point_size = 3, axis_label_fontsize = 10,
plot3D = TRUE
)
#cvplot
ggsave(str_c(results.path,model.name,"_spatialcv.png"), cvplot, bg="white")
##specify the budget available for tuning - we use
##terminate after a given number of iterations of hyper-parameter combinations
evals = mlr3tuning::trm("evals", n_evals = nmod)
#Set the optimization of algorithm takes place - we choose random search
tuner = mlr3tuning::tnr("random_search")
# define the outer limits of the randomly selected hyper-parameters
search_space = paradox::ps(
# The number of trees in the model (each one built sequentially)
nrounds = paradox::p_int(lower = 100, upper = 300),
# number of splits in each tree
max_depth = paradox::p_int(lower = 1, upper = 6),
# Learning rate - "shrinkage" - prevents overfitting
eta = paradox::p_dbl(lower = .1, upper = .4),
min_child_weight = paradox::p_dbl(lower = 1, upper =10),
colsample_bytree = paradox::p_dbl(lower = 0.5, upper =1),
subsample = paradox::p_dbl(lower = 0.5, upper =1),
lambda = paradox::p_dbl(lower = -1, upper = 0, trafo = function(x) 10^x))
#Tuner for re-sampling only
at_resample <- mlr3tuning::AutoTuner$new(
learner = learner,
resampling = tune.level,
measure = mlr3::msr("classif.auc"),
search_space = search_space,
terminator = evals,
tuner = tuner
)
rm(list = ls(all.names = TRUE))
#Project defaults: Albers Equal Projection
project.crs <- 'EPSG:3577'
project.res <- 100
#Load required libraries
packages <- c("sf","terra","data.table","tidyverse",
"mlr3","mlr3learners","mlr3viz","mlr3tuning",
"iml","future","furrr","purrr","xgboost",
"lattice","tictoc","scico","ggtext","mlr3spatiotempcv")
#Install the old version of mlr3spatitempcv manually#
if(!require(devtools)) install.packages("devtools")
library(devtools)
if (!require("mlr3spatiotempcv")) install_version("mlr3spatiotempcv",
version = "1.0.1",
repos = "http://cran.us.r-project.org")
install.packages("https://cran.r-project.org/src/contrib/Archive/mlr3spatiotempcv/mlr3spatiotempcv_1.0.1.tar.gz",
repos=NULL, method = "libcurl")
library(R.utils)
install.packages("https://cran.r-project.org/src/contrib/Archive/mlr3spatiotempcv/mlr3spatiotempcv_1.0.1.tar.gz")
# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
install.packages(packages[!installed_packages])
}
#Load the packages
lapply(packages, require, character.only=TRUE)
######### Modelling parameters #################
#n-samples <- 100 #Samples # all samples are selected in the code below use this later if there is a need
nfolds <- 5 #CV folds
nreps <- 2 #Number of times to repeat CV
nmod <- 5 #Hyper parameter search limit
proportion_sample <- 0.2
##################################3 DON"T MODIFY ANYTHING BELOW THIS CODE ##########################
#Create a list of study area and bind them for loop
#Specify the data path based on the system
data.path <- case_when(
Sys.info()["sysname"] == "Windows" ~ "./data/",
Sys.info()["sysname"] == "Darwin" ~ "/Users/ascelin/tmp/NSW_cfac/data/",
Sys.info()["sysname"] == "Linux" ~ "/dev/shm/data/" #Amazon EC2
#Sys.info()["sysname"] == "Linux" ~ "/home/ubuntu/data/" #Nectar
)
#Warning message if it can't find the directory
if (dir.exists(data.path)){
print("Directory exists - process will run")
} else {
string.to.print <- paste("ERROR: can't find the directory: check data path", data.path)
stop(string.to.print)
}
nsw <- st_read(str_c(data.path,"studyarea/state/NSW_STATE_POLYGON_shp_ex_islands_proj.shp"))
combined_bio <- st_read(str_c(data.path,"studyarea/Cfact_analysis_regions/Cfact_analysis_regions_prj.shp"))
bioregion <- st_read(str_c(data.path,"studyarea/ibra/IBRA_NSW_clipped.shp"))
studyarea <- rbind(nsw %>% transmute(name = "state"),
combined_bio %>% transmute(name = combined_bio$Cfact_Regi),
bioregion %>% transmute(name = bioregion$REG_NAME_7))
print(studyarea$name)
yearmodelled <- "post2015"
yearlosskd <- "pre2015"
region <- "NSW North Coast"
agent <- "agri"
results.path <- case_when(
Sys.info()["sysname"] == "Windows" ~ str_c("./results/",region,"/"),
Sys.info()["sysname"] == "Darwin" ~ str_c("/Users/ascelin/tmp/NSW_cfac/results/",region,"/"),
Sys.info()["sysname"] == "Linux" ~ str_c("/dev/shm/output/",region,"/") #Amazon
#Sys.info()["sysname"] == "Linux" ~ str_c("/home/ubuntu/results/",region,"/") #Nectar
)
if (!dir.exists(results.path)){dir.create(results.path)}
print(str_c("Preparing data to run ", agent, " in ", region, " bioregion"))
loss.path <- dir(str_c(data.path, "loss"), full.names=T, pattern = str_c("majorityrule_",yearmodelled,".tif$"))
cov.path <- dir(str_c(data.path, "covariates", sep=""), full.names = T, pattern = ".tif$")
loss.path
cov.path <- dir(str_c(data.path, "covariates", sep=""), full.names = T, pattern = ".tif$")
#Remove minimum lot size from the state level as there is no data in western states
cov.path <- str_subset(cov.path, pattern = "minimumlotsize", negate = T)
cov.path <- str_subset(cov.path, pattern = "landuse", negate = T)
#Set up a directory for KD of previous losses
kd.path <- dir(str_c(data.path, "covariates/losskd", sep=""), full.names = T, pattern = ".tif$")
#pattern <- "landsat"
#Use both combine and separately for landsat and SPOT
pattern <- c("landsat",yearlosskd)
kd.path <- map(pattern, str_subset, string=kd.path) %>% unlist()
#Add to cov.path
cov.path <- c(cov.path,kd.path)
#Load BCT and WOODY DATA
bct <- st_read(dir(str_c(data.path,"/bct/"), full.names = T, pattern = "*.shp$"))
woody.private.land <- rast(dir(str_c(data.path, "woodyonprivateland", sep=""), full.names = T, pattern = ".tif$"))
#Create a function to run analysis across the parameters
roi <- studyarea %>% filter(name == region)
## 1.1 Get loss raster
loss.file <- case_when(
agent == "agri" ~ list(str_subset(loss.path,"agri")),
agent == "fores" ~ list(str_subset(loss.path, "fores")),
agent == "infra" ~ list(str_subset(loss.path,"infra")),
agent == "af" ~ list(c(str_subset(loss.path,"agri"),
str_subset(loss.path,"fores"))),
agent == "afi" ~ list(c(str_subset(loss.path,"agri"),
str_subset(loss.path,"fores"),
str_subset(loss.path,"infra"))))
loss.file <- unlist(loss.file)
loss.file <- unique(loss.file) #Remove duplicate filenames
loss.raster <- map(loss.file, rast)
#Merge the files in the raster
loss.raster <- do.call(merge, loss.raster)
loss.raster <- map(loss.file, rast)
plot(loss.raster)
## 1.1 Get loss raster
loss.file <- case_when(
agent == "agri" ~ list(str_subset(loss.path,"agri")),
agent == "fores" ~ list(str_subset(loss.path, "fores")),
agent == "infra" ~ list(str_subset(loss.path,"infra")),
agent == "af" ~ list(c(str_subset(loss.path,"agri"),
str_subset(loss.path,"fores"))),
agent == "afi" ~ list(c(str_subset(loss.path,"agri"),
str_subset(loss.path,"fores"),
str_subset(loss.path,"infra"))))
loss.file
loss.file <- unlist(loss.file)
loss.file
loss.file <- unique(loss.file) #Remove duplicate filenames
loss.raster <- map(loss.file, rast)
plot(loss.raster)
#Merge the files in the raster
loss.raster <- do.call(merge, loss.raster)
View(loss.raster)
#Crop to the ROI only if it is not a state model
ifelse(
region == "state",
loss.raster,
loss.raster <- loss.raster %>% crop(roi) %>% mask(vect(roi))
)
View(loss.raster)
agent <- "af"
loss.file <- unlist(loss.file)
loss.file <- unique(loss.file) #Remove duplicate filenames
loss.raster <- map(loss.file, rast)
loss.raster
View(loss.raster)
## 1.1 Get loss raster
loss.file <- case_when(
agent == "agri" ~ list(str_subset(loss.path,"agri")),
agent == "fores" ~ list(str_subset(loss.path, "fores")),
agent == "infra" ~ list(str_subset(loss.path,"infra")),
agent == "af" ~ list(c(str_subset(loss.path,"agri"),
str_subset(loss.path,"fores"))),
agent == "afi" ~ list(c(str_subset(loss.path,"agri"),
str_subset(loss.path,"fores"),
str_subset(loss.path,"infra"))))
loss.file
loss.file <- unlist(loss.file)
loss.file
loss.file <- unique(loss.file) #Remove duplicate filenames
loss.file
loss.raster <- map(loss.file, rast)
loss.raster
#Crop to the ROI only if it is not a state model
ifelse(
region == "state",
loss.raster,
loss.raster <- loss.raster %>% crop(roi) %>% mask(vect(roi))
)
#Merge the files in the raster
loss.raster <- do.call(merge, loss.raster)
#Crop to the ROI only if it is not a state model
ifelse(
region == "state",
loss.raster,
loss.raster <- loss.raster %>% crop(roi) %>% mask(vect(roi))
)
plot(loss.raster)
plot(loss.raster)
loss.raster <- map(loss.file, rast)
#Merge the files in the raster
nrow(loss.raster)
#Merge the files in the raster
len(loss.raster)
#Merge the files in the raster
length(loss.raster)
loss.raster <- map(loss.file, rast)
#Merge the files in the raster
ifelse(
length(loss.raster) > 1,
loss.raster <- do.call(merge, loss.raster),
loss.raster <- unlist(loss.raster))
plot(loss.raster)
agent <- "agri"
## 1.1 Get loss raster
loss.file <- case_when(
agent == "agri" ~ list(str_subset(loss.path,"agri")),
agent == "fores" ~ list(str_subset(loss.path, "fores")),
agent == "infra" ~ list(str_subset(loss.path,"infra")),
agent == "af" ~ list(c(str_subset(loss.path,"agri"),
str_subset(loss.path,"fores"))),
agent == "afi" ~ list(c(str_subset(loss.path,"agri"),
str_subset(loss.path,"fores"),
str_subset(loss.path,"infra"))))
loss.file
loss.file <- unlist(loss.file)
loss.file
loss.file <- unique(loss.file) #Remove duplicate filenames
loss.file
loss.raster <- map(loss.file, rast)
loss.raster
#Merge the files in the raster
ifelse(
length(loss.raster) > 1,
loss.raster <- do.call(merge, loss.raster),
loss.raster <- unlist(loss.raster))
View(loss.raster)
length(loss.raster)
#Merge the files in the raster
ifelse(
length(loss.raster) > 2,
loss.raster <- do.call(merge, loss.raster),
loss.raster <- unlist(loss.raster))
loss.raster <- unlist(loss.raster)
View(loss.raster)
?unlist
loss.raster <- do.call(rbind, loss.raster)
?flatten
loss.raster <- flatten(loss.raster)
View(loss.raster)
#Crop to the ROI only if it is not a state model
ifelse(
region == "state",
loss.raster,
loss.raster <- loss.raster %>% crop(roi) %>% mask(vect(roi))
)
unlist(loss.raster)
#Merge the files in the raster
ifelse(
length(loss.raster) > 1,
loss.raster <- do.call(merge, loss.raster),
loss.raster <- rast(loss.raster))
#Crop to the ROI only if it is not a state model
ifelse(
region == "state",
loss.raster,
loss.raster <- loss.raster %>% crop(roi) %>% mask(vect(roi))
)
plot(loss.raster)
roi
#Clip Loss Raster
all.losses <- rast(str_c(data.path,"loss/", "nsw_state_allagents_resampled100m_majorityrule.tif"))
ifelse(
region == "state",
all.losses,
all.losses <- all.losses %>% crop(roi) %>% mask(vect(roi))
)
woodyonprivate <- rast(str_c(data.path, "woodyonprivateland/","woodyonprivate.tif"))
ifelse(
region == "state",
woodyonprivate,
woodyonprivate <- woodyonprivate %>% crop(roi) %>% mask(vect(roi))
)
loss.pts <- as.points(loss.raster) %>%
geom() %>%
as.data.frame() %>%
transmute(x,y)
totallosspts <- nrow(loss.pts)
nsamples <- as.numeric(nrow(loss.pts))
nsamples <- case_when(
nrow(loss.pts) < nsamples ~ nrow(loss.pts),
TRUE ~ as.integer(nsamples)
)
## 2. Collect background samples
bg.raster <- woodyonprivate %>%
mask(., vect(bct), inverse = T) %>%
mask(., all.losses, inverse = T)
loss.pts <- loss.pts %>%
slice_sample(n=nsamples)
set.seed = 2022
bg.pts <- as.points(bg.raster) %>%
geom() %>%
as.data.frame() %>%
transmute(x,y) %>%
slice_sample(n = nrow(loss.pts))
## 3. Join loss and background points 1= loss and 0 = background
pts <- rbind(
loss.pts %>% mutate(loss = as.factor(1)),
bg.pts %>% mutate(loss = as.factor(0)))
##3.1 Convert to sf object
pts <- st_as_sf(pts, coords= c("x","y"))
model.name <- str_c("roi_",region,"_",
"agent_",agent,"_",
"yr_",yearmodelled,"_",
"samples_",nrow(loss.pts),"_",
"cv_",nfolds,"_",
"rep_",nreps,"_",
"mod_",nmod)
## 4.4 Covariates data
covariates <- rast(cov.path)
ifelse(
region == "state",
covariates,
covariates <- covariates %>% crop(roi) %>% mask(vect(roi))
)
print(str_c("No of covariates used for modelling: ", nlyr(covariates)))
#extract names from file_names
good.names <- str_extract(cov.path,pattern = "(?<=_)[^.]*(?=.)")
good.names
#Assign names now
names(covariates) <- good.names
df <- terra::extract(covariates,vect(pts), xy=T) %>%
data.frame() %>%
mutate(loss = as.factor(pts$loss)) %>%
dplyr::select(-ID)%>% #At this step check which points have NA -
drop_na() #mlr doesn't take na so remove if any column has NA
print(str_c("Data preparation complete for model:: ",model.name))
################# STEP 2: MODELLING SECTION ###########################
print(str_c("Model training starting for ", agent, " in ", region, " bioregion"))
task = mlr3spatiotempcv::TaskClassifST$new(
id = "xgboost_model",
backend = mlr3::as_data_backend(df),
target = "loss",
positive = "1",
extra_args = list(
coordinate_names = c("x", "y"), #Specify to use these columns as coordinates
coords_as_features = FALSE,
crs = "EPSG:3577") #Albers
)
levs = levels(task$truth())
## Step 2.2 Choose a learner and set predict output in probability
learner = mlr3::lrn("classif.xgboost",predict_type = "prob")
toc(log = TRUE, quiet = TRUE)
log.txt <- unlist(tic.log(format = T))
tic.clearlog()
#Save all model details in a dataframe
model_details <- data.frame(
roi = region,
samples = nsamples,
totalloss_pixels = totallosspts,
cv = nfolds,
mods = nmod,
rep = nreps,
agent = agent,
auc_mean = auc.mean,
auc_dist = I(list(auc$classif.auc)),
time = log.txt)
#Save all model details in a dataframe
model_details <- data.frame(
roi = region,
samples = nsamples,
totalloss_pixels = totallosspts,
cv = nfolds,
mods = nmod,
rep = nreps,
agent = agent,
auc_mean = auc.median,
auc_dist = I(list(auc$classif.auc)),
time = log.txt)
auc.median <- 90
#Save all model details in a dataframe
model_details <- data.frame(
roi = region,
samples = nsamples,
totalloss_pixels = totallosspts,
cv = nfolds,
mods = nmod,
rep = nreps,
agent = agent,
auc_mean = auc.median,
auc_dist = I(list(auc$classif.auc)),
time = log.txt)
region <- "NSW North Coast"
agent <- "agri"
task = as_task_classif_st(df, id = "xgboost_model", target = "loss",
positive = "1", coordinate_names = c("x", "y"), crs = 3577)
levs = levels(task$truth())
## Step 2.2 Choose a learner and set predict output in probability
learner = mlr3::lrn("classif.xgboost",predict_type = "prob")
task = mlr3spatiotempcv::TaskClassifST$new(
id = "xgboost_model",
backend = mlr3::as_data_backend(df),
target = "loss",
positive = "1",
extra_args = list(
coordinate_names = c("x", "y"), #Specify to use these columns as coordinates
coords_as_features = FALSE,
crs = "EPSG:3577") #Albers
)
task = as_task_classif_st(df, id = "xgboost_model", target = "loss",
positive = "1", coordinate_names = c("x", "y"), crs = 3577)
